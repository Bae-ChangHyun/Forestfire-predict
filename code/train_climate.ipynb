{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ef9193",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-27T09:16:06.177918Z",
     "start_time": "2023-04-27T09:16:06.161561Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "import requests\n",
    "import natsort\n",
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 제거\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import tifffile\n",
    "import rasterio\n",
    "import geopandas as gpd # geo 데이터프레임 만들기 위함\n",
    "from pyidw import idw # 보간법 실행\n",
    "from osgeo import gdal # 래스터 자르기 anaconda로 설치\n",
    "from rasterio.plot import show # 래스터 시각화\n",
    "from shapely.geometry import Point\n",
    "from rasterio.features import geometry_mask\n",
    "from rasterio.transform import from_origin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a07f2c1",
   "metadata": {},
   "source": [
    "### asos 크롤링 funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf749e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asos_crawling(date,locn):\n",
    "    \"\"\" \n",
    "    - asos 기상 데이터를 크롤링\n",
    "    - 최대 5번까지 시도하는 방식\n",
    "    \"\"\"\n",
    "    url = 'http://apis.data.go.kr/1360000/AsosHourlyInfoService/getWthrDataList' # 크롤링할 주소 \n",
    "\n",
    "    startDt=datetime.strptime(date[:8], '%Y%m%d') # 시작날짜\n",
    "    startHh = datetime.strptime(date[8:], '%H')   # 시작시간\n",
    "    endHh = (startHh + timedelta(hours=1))        # 시작날짜 + 1 (23같은경우 00이 되게 하기위해서 timedelta 이용)\n",
    "    if(endHh.strftime('%H:%M:%S').split(':')[0]=='00'):endDt=(startDt + timedelta(days=1))\n",
    "    else:endDt=startDt # endhh==00이면 시작날짜와 종료날짜가 달라야함. 22일 23시와 23일 00시 이런식.\n",
    "\n",
    "    startDt=startDt.strftime('%Y%m%d')\n",
    "    startHh=startHh.strftime('%H:%M:%S').split(':')[0]\n",
    "    endDt=endDt.strftime('%Y%m%d')\n",
    "    endHh=endHh.strftime('%H:%M:%S').split(':')[0]\n",
    "\n",
    "    # Servicekey list\n",
    "    # pXv2lwkQOlbcBekMvEjur8cGaDpQb84Upv6ZboITLKGudf2//PqPjL6QABwMtAhzNilIwndbgQdo9lBTUwy3XA==\n",
    "    # 1tZqzN7UEB+yrAZ61++roasr+iAGFTX9QhDLFqpYOw7oDYHKMhRjCsk5jy8YAQz+xBdStnCvi4rQ/0SITX2cEg==\n",
    "    # 3imQf/ygL+vTqRcXZ19hAwVhJhVDxZ2yRGtaRQPk/F3rFSVB2Kvu7LFfoGVhB4rYfTVk2kILGAhhJvmu9kQUzA==  : 아빠 \n",
    "    params ={'serviceKey' : '1tZqzN7UEB+yrAZ61++roasr+iAGFTX9QhDLFqpYOw7oDYHKMhRjCsk5jy8YAQz+xBdStnCvi4rQ/0SITX2cEg==', #AuthenticationKey\n",
    "                'pageNo' : '1',\n",
    "                'numOfRows' : '10',\n",
    "                'dataType' : 'JSON', \n",
    "                'dataCd' : 'ASOS', \n",
    "                'dateCd' : 'HR', \n",
    "                'startDt' : startDt, #startdate\n",
    "                'startHh' : startHh, #starttime\n",
    "                'endDt' : endDt, # end date\n",
    "                'endHh' : endHh, # end time\n",
    "                'stnIds' : locn \n",
    "            }\n",
    "    for i in range(5):  # 최대 5번까지 시도\n",
    "        try:\n",
    "            response = requests.get(url, params=params,verify=False)\n",
    "            #print(response.url) # 요청 url 출력하기 \n",
    "            try:\n",
    "                json_obj = json.loads(response.content)\n",
    "                try:\n",
    "                    json_obj=json_obj[\"response\"][\"body\"][\"items\"][\"item\"][0]\n",
    "                    times=json_obj['tm']\n",
    "                    humidity=json_obj['hm']\n",
    "                    windspeed=json_obj['ws']\n",
    "                    rain=json_obj['rn']\n",
    "                    temp=json_obj['ta']              \n",
    "                    return times, humidity, windspeed, rain, temp\n",
    "                except:\n",
    "                    #if(json_obj[\"response\"]['header']['resultMsg']=='NO_DATA'):\n",
    "                        #print(\"No_Data\")\n",
    "                        #return np.nan,np.nan,np.nan,np.nan,np.nan,np.nan\n",
    "                    #else:\n",
    "                        #print(\"Retry\")\n",
    "                    print(\"Retry1\")\n",
    "                    print(json_obj)\n",
    "                    time.sleep(5)\n",
    "            except:\n",
    "                print(response.content)\n",
    "                try:\n",
    "                    json_obj = json.loads(response.content)\n",
    "                    print(response,json_obj)\n",
    "                    print(\"Retry2\")\n",
    "                    time.sleep(5)\n",
    "                except json.JSONDecodeError:\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "            # 네트워크 연결 끊겼을 때.    \n",
    "        except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as e:\n",
    "            print(f'오류 발생 재시도')\n",
    "            time.sleep(2)\n",
    "        #print(response.content)\n",
    "        # 현재는 시간,습도,풍속,강수량,기온 \n",
    "    return np.nan,np.nan,np.nan,np.nan,np.nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "00dabbc0",
   "metadata": {},
   "source": [
    "### 기상 데이터 수집 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437169e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T05:27:24.478726Z",
     "start_time": "2023-04-25T05:27:24.448320Z"
    }
   },
   "outputs": [],
   "source": [
    "def crawl_data(data_n,o_data,method):\n",
    "    \"\"\"\n",
    "    method==1 \n",
    "        - api를 이용해서 직접 크롤링 하는 방식->새로운 데이터를 얻을 수 있지만, api 불안정하여 시간이 오래걸림\n",
    "        - 데이터 크롤링\n",
    "        - 산불발생 데이터에서 산불 발생 날짜 가져옴\n",
    "        - 산불 발생 날짜에 대해서 전 지역 asos 데이터를 크롤링\n",
    "        - 한번 크롤링 할 때 한 asos 지점에 대한 기상데이터를 가져오면서 합침 \n",
    "        - 최종으로 산불발생 개수 만큼 csv가 생성되고, 각 csv에는 산불발생시점 전 지역에 대한 기상데이터 존재 \n",
    "    \n",
    "    method==2\n",
    "        - 기존에 수집한 asos 시간별 데이터에서 가져옴->최신화 어렵고, 용량 차지하는데 크롤링보다는 빠르고 정확\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Start crawling asos clmate data\")\n",
    "    filepath=f\"../data/data_set({data_n})/climate_data/\"\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    \n",
    "    if((len(glob(os.path.join(filepath, \"*.csv\"))))==len(o_data)):\n",
    "        print(\"-->Already existed\")\n",
    "        print(\"---------------\")\n",
    "    else:\n",
    "        loc_data=pd.read_csv(\"../data/aws_loc_list.csv\") # asos 지점과 경위도 데이터 \n",
    "        loc_list=loc_data['지점번호']\n",
    "        result=[]\n",
    "        \n",
    "        if(method==1):\n",
    "            for i in tqdm(range(len(o_data))):\n",
    "                tmp=pd.DataFrame(columns=['num','loc_info', 'lon','lat','time','humidity','wind_sp','rainfall','temp'])\n",
    "                for j in range(len(loc_list)):\n",
    "                    new_data=[i,loc_list[j],loc_data['lon'][j],loc_data['lat'][j]]\n",
    "                    new_data.extend(asos_crawling(o_data['input'][i],loc_list[j])) \n",
    "                    tmp = pd.concat([tmp, pd.DataFrame([new_data], columns=tmp.columns)], ignore_index=True)\n",
    "                tmp.to_csv(f\"../data/data_set({data_n})/climate_data/data_{i}.csv\",encoding='cp949',index=False) \n",
    "            \n",
    "        if(method==2):\n",
    "            for i in tqdm(range(len(o_data))):\n",
    "                o_data['input'] = o_data['input'].astype(str)\n",
    "                year = o_data['input'][i][:4]\n",
    "                \n",
    "                df_asos_hr = pd.read_csv(f'../data/ASOS_Hr/ASOS_Hr_{year}.csv', low_memory=False)\n",
    "                \n",
    "                date=datetime.strptime(o_data['input'][i], \"%Y%m%d%H\")\n",
    "                date = date.strftime(\"%Y-%m-%d %H:%M\")\n",
    "                df_asos_hr = df_asos_hr[df_asos_hr['일시'] == date]\n",
    "                df_asos_hr = df_asos_hr.reset_index(drop=True)\n",
    "\n",
    "                df_asos_hr['num'] = i\n",
    "                df_asos_hr['longitude'] = [loc_data[loc_data['지점번호']==df_asos_hr['지점'][i_asos_hr]]['lon'].values[0] if loc_data[loc_data['지점번호']==df_asos_hr['지점'][i_asos_hr]]['lon'].shape[0]!=0 else np.nan for i_asos_hr in range(df_asos_hr.shape[0])]\n",
    "                df_asos_hr['latitude'] = [loc_data[loc_data['지점번호']==df_asos_hr['지점'][i_asos_hr]]['lat'].values[0] if loc_data[loc_data['지점번호']==df_asos_hr['지점'][i_asos_hr]]['lat'].shape[0]!=0 else np.nan for i_asos_hr in range(df_asos_hr.shape[0])]\n",
    "                df_asos_hr = df_asos_hr[['num', '지점', 'longitude', 'latitude', '일시', '습도(%)', '풍속(m/s)', '강수량(mm)', '기온(°C)']]\n",
    "                df_asos_hr.columns=['num','loc_info','lon','lat','time','humidity','wind_sp','rainfall','temp']\n",
    "                df_asos_hr.to_csv(f'../data/data_set({data_n})/climate_data/data_{i}.csv',encoding='cp949',index=False)\n",
    "                \n",
    "                \n",
    "        \"\"\"\n",
    "        최종적으로 수집한 각 산불별 기상데이터를 하나로 합치는 과정\n",
    "        \"\"\"\n",
    "        for i in range(len(o_data)):\n",
    "            tmp=pd.read_csv(f\"../data/data_set({data_n})/climate_data/data_{i}.csv\",encoding='cp949')\n",
    "            result.append(tmp)        \n",
    "        weather_data = pd.concat(result,ignore_index=True)\n",
    "        weather_data.to_csv(f\"../data/data_set({data_n})/final_climate_data.csv\",encoding='cp949',index=False)\n",
    "        print(\"-->Complete\")\n",
    "        print(\"---------------\")\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65390fd7",
   "metadata": {},
   "source": [
    "### 기상 데이터 보간 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d29e4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_climate(data_n,o_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    - 전국의 모든 지역에 대한 기상데이터가 존재하지 않기 때문에,\n",
    "    asos의 모든 지점으로 부터 얻은 기상데이터를 이용하여, 각 feature별로 데이터를 보간(역거리가중법 / idw보간)\n",
    "    - 보간시 하나의 feature라도 없으면 전부 drop( 결측치 )\n",
    "    - 한 지점이라도 살아있으면 보간이 되서 우선 냅둠 -\n",
    "    # -->어떤 산불은 지점 여러개로 보간된 데이터, 어떤건 지점 한,두개로 보간된 데이터\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Start climate interpolation\")\n",
    "    filepath=f\"../data/data_set({data_n})/interpolate_climate/\"\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    \n",
    "    if(len(glob(filepath + '/*')))!=4: # 4개의 폴더가 있어야 함\n",
    "\n",
    "        features=['humidity','wind_sp','rainfall','temp']\n",
    "        \n",
    "        os.makedirs(filepath+features[0], exist_ok=True)\n",
    "        os.makedirs(filepath+features[1], exist_ok=True)\n",
    "        os.makedirs(filepath+features[2], exist_ok=True)\n",
    "        os.makedirs(filepath+features[3], exist_ok=True)\n",
    "\n",
    "        weather_data=pd.read_csv(f\"../data/data_set({data_n})/final_climate_data.csv\",encoding='cp949')\n",
    "        weather_data.columns=['num','loc_info','lon','lat','time','humidity','wind_sp','rainfall','temp']\n",
    "        weather_data.dropna(subset=['time'], inplace=True) # time이 null이면 데이터가 정상적으로 크롤링되지 않은 것임. \n",
    "        weather_data['rainfall']=weather_data['rainfall'].fillna(0) # 강수가 비어있는건 0으로 채움 \n",
    "\n",
    "        for i in tqdm(range(len(o_data))):\n",
    "            tmp=weather_data[weather_data['num']==i]\n",
    "            tmp=tmp.dropna()\n",
    "            # 결측치 전부 drop 후에 데이터가 없으면 보간하지 않음\n",
    "            if(len(tmp)==0):\n",
    "                print(\"There is no data\")\n",
    "                continue\n",
    "            tmp.drop(['num','loc_info','time'],axis=1,inplace=True)\n",
    "            tmp = gpd.GeoDataFrame(tmp, geometry=gpd.points_from_xy(tmp.lon, tmp.lat))\n",
    "            tmp.to_file(f'data{i}.shp')\n",
    "            \n",
    "            for j in range(len(features)):\n",
    "                idw.idw_interpolation(\n",
    "                    input_point_shapefile=f'data{i}.shp', # 보간하고자 하는 shp 파일 \n",
    "                    extent_shapefile=\"../data/gw_boundary/boundary.shp\", # 경계 shp 파일(현재 강원도)\n",
    "                    column_name=features[j], # 보간하고자 하는 feature 이름. \n",
    "                    power=2, # 거리 가중치 계수 \n",
    "                    search_radious=8, # 검색하고자 하는 범위 \n",
    "                    output_resolution=400, # 결과물 해상도 \n",
    "                )\n",
    "                image=rasterio.open(f\"data{i}_idw.tif\")\n",
    "                image=pd.DataFrame(image.read(1))\n",
    "                image.to_csv(f\"{filepath}{features[j]}/data{i}_idw.csv\",encoding='cp949')\n",
    "                os.remove(f\"data{i}_idw.tif\")\n",
    "            \n",
    "            os.remove(f\"data{i}.shp\")\n",
    "            os.remove(f\"data{i}.cpg\")\n",
    "            os.remove(f\"data{i}.dbf\")\n",
    "            os.remove(f\"data{i}.shx\")\n",
    "            print(\"-->Complete\")\n",
    "            print(\"---------------\")\n",
    "    else:\n",
    "        print(\"-->Already existed\")\n",
    "        print(\"---------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98dc5705",
   "metadata": {},
   "source": [
    "### 산불 발생 위치 탐색 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe3c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_fireloc(data_n,o_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    다른 기상데이터는 필요없고, 산불 발생 위치의 기상데이터만 필요하기 때문에 산불 발생 위치를 찾는 코드\n",
    "    이미 산불 발생 위치의 경위도는 알고 있지만, 보간된 기상데이터에서 산불 발생 위치의 데이터를 가져오기 위해선\n",
    "    아래와 같이 생성한 shp를 tif로 변환하면서, 강원도 밖은 32767 강원도 내부는 -9999, 산불 발생 위치는 624로 채움\n",
    "    --> 추후,다른 부분의 기상이 필요하지 않다면, 기상청만큼 신뢰성 있는 사이트에서 기상데이터 한개만을 가져오는 방식이 훨씬 효율적\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Start find fire loc\")\n",
    "    filepath=f\"../data/data_set({data_n})/fire_loc/\"\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "    \n",
    "    if(len(glob(filepath + '/*')))==0:\n",
    "        for i in tqdm(range(len(o_data))):\n",
    "            tmp=pd.DataFrame(o_data.iloc[i])\n",
    "            tmp=tmp.T\n",
    "            tmp = gpd.GeoDataFrame(tmp, geometry=gpd.points_from_xy(tmp.lon, tmp.lat))\n",
    "            tmp.drop(['date','time','lon','lat','input'],axis=1,inplace=True)\n",
    "            tmp.to_file('fire_data.shp')\n",
    "\n",
    "            shp_path  = \"fire_data.shp\" # 현재 shp파일 이름 \n",
    "            boundary_path='../data/gw_boundary/boundary.shp'\n",
    "            tif_path  = f\"fire_data.tif\" # 만들고자 하는 tif파일 이름\n",
    "\n",
    "            driver = gdal.GetDriverByName('GTiff')\n",
    "            # 산불 shp 파일을 읽어오기\n",
    "            shp_datasource = gdal.OpenEx(shp_path, gdal.OF_VECTOR)\n",
    "            # 경계shp 파일을 읽어오기\n",
    "            boundary_datasource = gdal.OpenEx(boundary_path, gdal.OF_VECTOR)\n",
    "            # tif 파일 생성\n",
    "            tif_datasource = driver.Create(tif_path, 400, 278, 1, gdal.GDT_Float32)\n",
    "            # 좌표계 설정\n",
    "            tif_datasource.SetProjection(shp_datasource.GetProjection())\n",
    "            # 강원도 경계값 가져옴 \n",
    "            boundary = gpd.read_file(boundary_path)\n",
    "            xmin, ymin, xmax, ymax = boundary.total_bounds\n",
    "            # 강원도 경계값 가져온걸 위에서 설정한 400,278 즉 액셀 파일 크기형태로 설정 \n",
    "            tif_datasource.SetGeoTransform((xmin, (xmax-xmin)/400, 0, ymax, 0, -(ymax-ymin)/278))\n",
    "            band = tif_datasource.GetRasterBand(1)\n",
    "        #   산불 난 지점 외에는 전부 32767으로 설정\n",
    "            band.Fill(32767)\n",
    "            band.SetNoDataValue(32767)\n",
    "            # 강원도 내부는 -9999로 채우기\n",
    "            gdal.RasterizeLayer(tif_datasource, [1], boundary_datasource.GetLayer(), burn_values=[-9999], options=[\"ALL_TOUCHED=TRUE\"])\n",
    "            # 산불 발생 위치는 624로 설정\n",
    "            gdal.RasterizeLayer(tif_datasource, [1], shp_datasource.GetLayer(), burn_values=[624]) \n",
    "            #gdal.RasterizeLayer(tif_datasource, [1], boundary_datasource.GetLayer(), burn_values=[32767])\n",
    "            shp_datasource = None\n",
    "            tif_datasource = None\n",
    "            tif_file = rasterio.open(f\"fire_data.tif\") \n",
    "            data = tifffile.imread(f\"fire_data.tif\")    \n",
    "            data=pd.DataFrame(data)\n",
    "            data.to_csv(f\"{filepath}fire_data{i}.csv\")\n",
    "            tif_file.close()\n",
    "            \n",
    "        # 필요없는 파일 삭제\n",
    "        os.remove(f\"fire_data.shp\")\n",
    "        os.remove(f\"fire_data.cpg\")\n",
    "        os.remove(f\"fire_data.dbf\")\n",
    "        os.remove(f\"fire_data.shx\")\n",
    "        os.remove(f\"fire_data.tif\")\n",
    "        print(\"-->Complete\")\n",
    "        print(\"---------------\")\n",
    "    else:\n",
    "        print(\"-->Already existed\")\n",
    "        print(\"---------------\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35a1a4c9",
   "metadata": {},
   "source": [
    "#### 산불 발생 위치 기상데이터 수집 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff6d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_climate_info(data_n,o_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    - 보간한 기상데이터에서 산불발생위치의 기상 데이터만을 가져옴.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Start get climate information\")\n",
    "    filepath=f\"../data/data_set({data_n})/train_{data_n}.csv\"\n",
    "    \n",
    "    if os.path.isfile(filepath):\n",
    "        print(\"Already existed\")\n",
    "        print(\"---------------\")\n",
    "    else:\n",
    "        climate=[]\n",
    "        for i in tqdm(range(len(o_data))):\n",
    "            data=pd.read_csv(f\"../data/data_set({data_n})/fire_loc/fire_data{i}.csv\")\n",
    "            index = data[data == 624].stack().index[0] # 산불발생위치를 찾음 \n",
    "            temp=pd.read_csv(f\"../data/data_set({data_n})/interpolate_climate/temp/data{i}_idw.csv\")\n",
    "            rain=pd.read_csv(f\"../data/data_set({data_n})/interpolate_climate/rainfall/data{i}_idw.csv\")\n",
    "            hums=pd.read_csv(f\"../data/data_set({data_n})/interpolate_climate/humidity/data{i}_idw.csv\")\n",
    "            wind=pd.read_csv(f\"../data/data_set({data_n})/interpolate_climate/wind_sp/data{i}_idw.csv\")\n",
    "            \n",
    "            r,c=index[0],int(index[1])\n",
    "            tmp_data = [\n",
    "                    [a, b, c, d]\n",
    "                    for a, b, c, d in zip(\n",
    "                        [temp.iloc[r][c]],\n",
    "                        [rain.iloc[r][c]],\n",
    "                        [hums.iloc[r][c]],\n",
    "                        [wind.iloc[r][c]]\n",
    "                    )\n",
    "                ]\n",
    "            climate.append(tmp_data[0])\n",
    "        climate=pd.DataFrame(climate,columns=['기온','강수','습도','풍속'])\n",
    "        o_data.drop('input',axis=1,inplace=True)\n",
    "        trainfire=pd.concat([o_data,climate],axis=1,ignore_index=True)\n",
    "        trainfire.columns=['date','time','lon','lat','temp','rainfall','humidity','windspeed']  \n",
    "        trainfire.to_csv(filepath,index=False)\n",
    "        print(\"-->Complete\")\n",
    "        print(\"---------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2aa726ea",
   "metadata": {},
   "source": [
    "#### 산불 안난 트레인 셋 만들기 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a03c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_dataset(start_year, end_year, num_samples,filepath): \n",
    "    \n",
    "    \"\"\"\n",
    "    - 산불 난 데이터는 target==1로 실존 데이터이고, 산불 나지 않은 데이터는 target==0으로 임의로 만든 데이터 셋이다.\n",
    "    - 데이터 셋을 만들 떄, 어떻게 만드냐에 따라 매우 달라진다.\n",
    "    - 같은 날씨에 같은 지형이 들어가는 경우는 없어야 하며, 계절과 날짜 모두 랜덤으로 골고루게 되었다.\n",
    "    - 데이터는 반드시 강원도 이내에 있어야 한다.\n",
    "    - 현재 만든 방식은 \n",
    "      1. 같은 지점을 총 5번씩 다른 날짜에 뽑았음--> 228*5=1140개의 데이터셋 만듦 \n",
    "    \"\"\"\n",
    "    seasons = ['spring', 'fall', 'winter','else']\n",
    "    season_weights = [0.6, 0.2, 0.1, 0.1]  # 비율을 설정합니다. 봄: 0.3, 여름: 0.15, 가을: 0.35, 겨울: 0.2\n",
    "    \n",
    "    min_latitude,max_latitude = 37.03353708,38.61370931\n",
    "    min_longitude,max_longitude = 127.0950376, 129.359995\n",
    "\n",
    "    tmp = []\n",
    "    for _ in range(num_samples):\n",
    "        year = random.randint(start_year, end_year)\n",
    "        hour = random.randint(0, 23)\n",
    "        season = random.choices(seasons, weights=season_weights)[0]\n",
    "        \n",
    "        if season == 'spring':\n",
    "            month = random.randint(2, 5)\n",
    "        elif season == 'fall':\n",
    "            month = random.randint(11, 12)\n",
    "        elif season == 'winter':\n",
    "            month = random.randint(6, 10)\n",
    "        else:\n",
    "            month = random.randint(1, 1)\n",
    "            \n",
    "        day = random.randint(1, 28)\n",
    "        minute,second=0,0\n",
    "        time = pd.Timestamp(year, month, day, hour, minute, second)\n",
    "\n",
    "        tmp.append(time)\n",
    "        \n",
    "    dates_df=pd.DataFrame(tmp,columns=['Date'])\n",
    "    dates_df['date'] = pd.to_datetime(dates_df['Date']).dt.strftime('%Y%m%d')\n",
    "    dates_df['time'] = pd.to_datetime(dates_df['Date']).dt.strftime('%H%M%S')\n",
    "    \n",
    "    dates_df[\"date\"] = dates_df[\"date\"].str.zfill(8)\n",
    "    dates_df[\"time\"] = dates_df[\"time\"].str.zfill(6)\n",
    "    \n",
    "    dates_df.drop('Date',axis=1,inplace=True)\n",
    "    #dates_df['Year'] = pd.to_datetime(dates_df['Date']).dt.year\n",
    "    #dates_df['Month'] = pd.to_datetime(dates_df['Date']).dt.month\n",
    "    #dates_df['Day'] = pd.to_datetime(dates_df['Date']).dt.day\n",
    "    #dates_df.groupby('Month').count()  \n",
    "    \n",
    "    tmp=[]\n",
    "    \n",
    "    for _ in range(500): # 강원도 바깥에생성될 걸 대비핵서 넉넉하게 \n",
    "        while True:\n",
    "            latitude = random.uniform(min_latitude, max_latitude)\n",
    "            longitude = random.uniform(min_longitude, max_longitude)\n",
    "            coordinate = (latitude, longitude)\n",
    "            if coordinate not in tmp:\n",
    "                tmp.append((longitude, latitude))\n",
    "                break\n",
    "    df = pd.DataFrame(tmp, columns=['lon', 'lat'])\n",
    "    #plt.scatter(df.lon,df.lat) # 전체분포가 올바른지 확인 \n",
    "\n",
    "    df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon, df.lat))\n",
    "    df.crs = \"EPSG:4326\" # 좌표계 설정 \n",
    "    df.to_file(f'df.shp')\n",
    "    df = gpd.read_file('df.shp')\n",
    "    gangwon_df = gpd.read_file(\"../data/gw_boundary/boundary.shp\")\n",
    "    gangwon_boundary = gangwon_df.geometry.unary_union\n",
    "    df = df[df.geometry.within(gangwon_boundary)][:228]\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    os.remove('df.shp')\n",
    "    df.drop('geometry',axis=1,inplace=True)\n",
    "    df=pd.concat([df,df,df,df,df],axis=0,ignore_index=True)\n",
    "    \n",
    "    notfire=pd.concat([dates_df,df],axis=1)\n",
    "    \n",
    "    notfire['input'] = notfire['date'].astype(str)+notfire['time'].apply(lambda x: str(x)[:2]).str.zfill(2).astype(str)\n",
    "    \n",
    "    notfire.to_csv(filepath,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2251cff2",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "655ed0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fire_dataset(data_n):\n",
    "    filepath=f\"../data/gangwon_{data_n}.csv\"\n",
    "    if os.path.isfile(filepath)==False: # nofire 데이터가 없을 경우 생성해야함. \n",
    "        make_random_dataset(2011, 2022, 1140,filepath)\n",
    "    o_data=pd.read_csv(f\"../data/gangwon_{data_n}.csv\")\n",
    "    print(\"#\"*50)\n",
    "    print(f\"Make {data_n} dataset \")\n",
    "    weather_data=crawl_data(data_n,o_data,2)\n",
    "    interpolate_climate(data_n,o_data)\n",
    "    find_fireloc(data_n,o_data)\n",
    "    get_climate_info(data_n,o_data)\n",
    "    dataset=pd.read_csv(f\"../data/data_set({data_n})/train_{data_n}.csv\")\n",
    "    return dataset\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8b22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Make fire dataset \n",
      "Start crawling asos clmate data\n",
      "-->Already existed\n",
      "---------------\n",
      "Start climate interpolation\n",
      "-->Already existed\n",
      "---------------\n",
      "Start find fire loc\n",
      "-->Already existed\n",
      "---------------\n",
      "Start get climate information\n",
      "Already existed\n",
      "---------------\n",
      "##################################################\n",
      "Make nofire dataset \n",
      "Start crawling asos clmate data\n",
      "-->Already existed\n",
      "---------------\n",
      "Start climate interpolation\n",
      "-->Already existed\n",
      "---------------\n",
      "Start find fire loc\n",
      "-->Already existed\n",
      "---------------\n",
      "Start get climate information\n",
      "Already existed\n",
      "---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>temp</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20110122</td>\n",
       "      <td>233500</td>\n",
       "      <td>128.869839</td>\n",
       "      <td>37.782030</td>\n",
       "      <td>-2.800407</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>41.980458</td>\n",
       "      <td>3.389697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20110201</td>\n",
       "      <td>210700</td>\n",
       "      <td>128.852520</td>\n",
       "      <td>37.792612</td>\n",
       "      <td>1.070496</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>54.996640</td>\n",
       "      <td>1.931208</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20110226</td>\n",
       "      <td>164200</td>\n",
       "      <td>127.955332</td>\n",
       "      <td>37.379769</td>\n",
       "      <td>13.438066</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>23.056794</td>\n",
       "      <td>1.431839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20110212</td>\n",
       "      <td>61800</td>\n",
       "      <td>127.879749</td>\n",
       "      <td>37.220708</td>\n",
       "      <td>-6.431949</td>\n",
       "      <td>0.01229</td>\n",
       "      <td>47.088550</td>\n",
       "      <td>1.842775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20110211</td>\n",
       "      <td>175000</td>\n",
       "      <td>128.128864</td>\n",
       "      <td>37.772593</td>\n",
       "      <td>1.532300</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>55.537041</td>\n",
       "      <td>2.576569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>20180316</td>\n",
       "      <td>230000</td>\n",
       "      <td>129.034403</td>\n",
       "      <td>37.227534</td>\n",
       "      <td>-5.149554</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>85.426590</td>\n",
       "      <td>0.777338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2189</th>\n",
       "      <td>20210111</td>\n",
       "      <td>210000</td>\n",
       "      <td>128.028175</td>\n",
       "      <td>37.885594</td>\n",
       "      <td>-8.744794</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>69.356481</td>\n",
       "      <td>0.328502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>20140201</td>\n",
       "      <td>0</td>\n",
       "      <td>127.941031</td>\n",
       "      <td>37.395255</td>\n",
       "      <td>-0.415539</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>72.227057</td>\n",
       "      <td>0.308869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>20180505</td>\n",
       "      <td>190000</td>\n",
       "      <td>128.302668</td>\n",
       "      <td>38.227089</td>\n",
       "      <td>21.923886</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>25.347899</td>\n",
       "      <td>3.725006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>20200506</td>\n",
       "      <td>10000</td>\n",
       "      <td>128.387882</td>\n",
       "      <td>37.471189</td>\n",
       "      <td>11.566483</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>94.483936</td>\n",
       "      <td>0.796096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2193 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date    time         lon        lat       temp  rainfall   humidity   \n",
       "0     20110122  233500  128.869839  37.782030  -2.800407   0.00000  41.980458  \\\n",
       "1     20110201  210700  128.852520  37.792612   1.070496   0.00000  54.996640   \n",
       "2     20110226  164200  127.955332  37.379769  13.438066   0.00000  23.056794   \n",
       "3     20110212   61800  127.879749  37.220708  -6.431949   0.01229  47.088550   \n",
       "4     20110211  175000  128.128864  37.772593   1.532300   0.00000  55.537041   \n",
       "...        ...     ...         ...        ...        ...       ...        ...   \n",
       "2188  20180316  230000  129.034403  37.227534  -5.149554   0.00000  85.426590   \n",
       "2189  20210111  210000  128.028175  37.885594  -8.744794   0.00000  69.356481   \n",
       "2190  20140201       0  127.941031  37.395255  -0.415539   0.00000  72.227057   \n",
       "2191  20180505  190000  128.302668  38.227089  21.923886   0.00000  25.347899   \n",
       "2192  20200506   10000  128.387882  37.471189  11.566483   0.00000  94.483936   \n",
       "\n",
       "      windspeed  target  \n",
       "0      3.389697       1  \n",
       "1      1.931208       1  \n",
       "2      1.431839       1  \n",
       "3      1.842775       1  \n",
       "4      2.576569       1  \n",
       "...         ...     ...  \n",
       "2188   0.777338       0  \n",
       "2189   0.328502       0  \n",
       "2190   0.308869       0  \n",
       "2191   3.725006       0  \n",
       "2192   0.796096       0  \n",
       "\n",
       "[2193 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fire_dataset=make_fire_dataset(\"fire\")\n",
    "fire_dataset['target']=1\n",
    "nofire_dataset=make_fire_dataset(\"nofire\")\n",
    "nofire_dataset['target']=0\n",
    "final_dataset=pd.concat([fire_dataset,nofire_dataset],axis=0,ignore_index=True)\n",
    "final_dataset\n",
    "final_dataset.to_csv(\"../data/train_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b06cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forest_fire",
   "language": "python",
   "name": "forest_fire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
